<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <title>Optical Flow Sphere Camera</title>
    <style>
        body { margin: 0; overflow: hidden; background: #000; font-family: sans-serif; }
        
        /* The main view container */
        #container { position: relative; width: 100vw; height: 100vh; }

        /* Layers */
        video { position: absolute; top: 0; left: 0; width: 100%; height: 100%; object-fit: cover; z-index: 1; opacity: 0; }
        canvas#output-canvas { position: absolute; top: 0; left: 0; width: 100%; height: 100%; object-fit: cover; z-index: 2; }
        #three-canvas { position: absolute; top: 0; left: 0; width: 100%; height: 100%; z-index: 3; pointer-events: none; }
        
        /* UI Layer */
        #ui {
            position: absolute; top: 0; left: 0; width: 100%; height: 100%; z-index: 10;
            pointer-events: none; display: flex; flex-direction: column; justify-content: space-between; align-items: center;
        }

        #loading {
            position: absolute; top: 50%; left: 50%; transform: translate(-50%, -50%);
            color: #00d2d3; font-size: 24px; font-weight: bold; background: rgba(0,0,0,0.8);
            padding: 20px; border-radius: 10px; z-index: 100; text-align: center;
        }

        .hud-top {
            margin-top: 20px; text-align: center;
            background: rgba(0, 0, 0, 0.5); padding: 10px 20px; border-radius: 20px;
            color: white; pointer-events: auto;
        }

        /* The Green Capture Button */
        #btn-capture {
            width: 80px; height: 80px;
            background-color: transparent;
            border: 5px solid #2ecc71;
            border-radius: 50%;
            margin-bottom: 40px;
            cursor: pointer;
            pointer-events: auto;
            position: relative;
            transition: transform 0.1s;
        }
        #btn-capture::after {
            content: ''; position: absolute;
            top: 50%; left: 50%; transform: translate(-50%, -50%);
            width: 60px; height: 60px; background: #2ecc71; border-radius: 50%;
        }
        #btn-capture:active { transform: scale(0.9); }

        /* View Mode UI */
        #view-controls {
            display: none; pointer-events: auto; position: absolute; bottom: 30px; gap: 20px; z-index: 20;
        }
        button.action-btn {
            background: #3498db; color: white; border: none; padding: 10px 20px;
            font-size: 16px; border-radius: 5px; cursor: pointer;
        }

        /* Helper reticle */
        .reticle {
            position: absolute; top: 50%; left: 50%; transform: translate(-50%, -50%);
            width: 200px; height: 200px; border: 2px dashed rgba(255,255,255,0.5);
            pointer-events: none;
        }
        
        /* Tracking dots debugging (optional, normally hidden) */
        .tracking-status { font-size: 12px; color: #aaa; margin-top: 5px; }

    </style>
</head>
<body>

    <div id="loading">Loading Computer Vision Library...<br><span style="font-size:14px; color:#fff">Please wait (approx 5-10s)</span></div>

    <div id="container">
        <!-- Hidden source video -->
        <video id="webcam" playsinline muted autoplay></video>
        <!-- Canvas for OpenCV processing debugging (hidden visually usually, but keeping for structure) -->
        <canvas id="output-canvas" style="display:none"></canvas>
        <!-- ThreeJS overlay -->
        <div id="three-canvas"></div>

        <div id="ui">
            <div class="hud-top">
                <div id="msg">Initializing...</div>
                <div class="tracking-status" id="track-stats">Tracking Points: 0</div>
            </div>
            
            <div class="reticle"></div>

            <div id="capture-controls">
                <div id="btn-capture"></div>
            </div>

            <div id="view-controls">
                <button class="action-btn" id="btn-finish">Finish & View</button>
                <button class="action-btn" id="btn-reset" style="background:#e74c3c">Reset</button>
            </div>
        </div>
    </div>

    <!-- OpenCV.js -->
    <script async src="https://docs.opencv.org/4.8.0/opencv.js" onload="onOpenCvReady()" type="text/javascript"></script>

    <!-- Three.js Import -->
    <script type="importmap">
        {
            "imports": {
                "three": "https://unpkg.com/three@0.160.0/build/three.module.js",
                "three/addons/": "https://unpkg.com/three@0.160.0/examples/jsm/"
            }
        }
    </script>

    <script type="module">
        import * as THREE from 'three';
        import { OrbitControls } from 'three/addons/controls/OrbitControls.js';

        // --- Global Variables ---
        let video, threeCanvasContainer;
        let scene, camera, renderer, cubeMesh, wireframeMesh;
        let materials = [];
        let isCapturing = true; // True = Camera Mode, False = View Mode
        let capturedSegments = 0;

        // Tracking Globals
        let cvReady = false;
        let cap, oldGray, oldPoints, frameGray, p0, p1, st, err;
        let isTracking = false;
        const maxPoints = 50; // How many points to track
        
        // --- Initialization ---
        
        window.onOpenCvReady = function() {
            cvReady = true;
            document.getElementById('loading').innerHTML = "Starting Camera...";
            startCamera();
        };

        async function startCamera() {
            video = document.getElementById('webcam');
            try {
                const stream = await navigator.mediaDevices.getUserMedia({
                    video: { facingMode: 'environment', width: {ideal: 640}, height: {ideal: 480} },
                    audio: false
                });
                video.srcObject = stream;
                video.onloadedmetadata = () => {
                    video.play();
                    initThreeJS();
                    document.getElementById('loading').style.display = 'none';
                    document.getElementById('msg').innerText = "Look around (Move Slowly)";
                    // Start Processing Loop
                    requestAnimationFrame(processVideo);
                };
            } catch (e) {
                alert("Camera Error: " + e.message);
            }
        }

        // --- Three.js Setup ---

        function initThreeJS() {
            threeCanvasContainer = document.getElementById('three-canvas');
            
            scene = new THREE.Scene();
            // Transparent background so we see the video feed behind
            scene.background = null; 

            const aspect = window.innerWidth / window.innerHeight;
            camera = new THREE.PerspectiveCamera(75, aspect, 0.1, 1000);
            // Start inside the cube
            camera.position.set(0, 0, 0.1); 

            renderer = new THREE.WebGLRenderer({ alpha: true, antialias: true });
            renderer.setSize(window.innerWidth, window.innerHeight);
            threeCanvasContainer.appendChild(renderer.domElement);

            // Create the "Globe" (Cube)
            const geometry = new THREE.BoxGeometry(10, 10, 10);
            
            // Create materials. Initially invisible (transparent) so we see video through them
            // We add a wireframe helper to see the "slots"
            for (let i = 0; i < 6; i++) {
                const mat = new THREE.MeshBasicMaterial({ 
                    color: 0xffffff, 
                    side: THREE.BackSide,
                    transparent: true,
                    opacity: 0.1, // Very faint background
                    map: null 
                });
                mat.userData = { filled: false, id: i };
                materials.push(mat);
            }

            cubeMesh = new THREE.Mesh(geometry, materials);
            cubeMesh.scale.set(-1, 1, 1); // Invert so we are inside
            scene.add(cubeMesh);

            // Add Wireframe to guide user
            const wireGeo = new THREE.EdgesGeometry(geometry);
            const wireMat = new THREE.LineBasicMaterial({ color: 0x00ff00, linewidth: 2 });
            wireframeMesh = new THREE.LineSegments(wireGeo, wireMat);
            wireframeMesh.scale.set(-1, 1, 1);
            scene.add(wireframeMesh);

            // UI Listeners
            document.getElementById('btn-capture').addEventListener('click', captureFace);
            document.getElementById('btn-finish').addEventListener('click', finishAndSave);
            document.getElementById('btn-reset').addEventListener('click', () => location.reload());
        }

        // --- Optical Flow Tracking (The "Fake Gyro") ---

        function initTracking() {
            if (!cvReady) return;
            
            // OpenCV mat setup
            cap = new cv.VideoCapture(video);
            oldGray = new cv.Mat(video.videoHeight, video.videoWidth, cv.CV_8UC1);
            frameGray = new cv.Mat(video.videoHeight, video.videoWidth, cv.CV_8UC1);
            
            p0 = new cv.Mat(); // Previous points
            p1 = new cv.Mat(); // Current points
            st = new cv.Mat(); // Status
            err = new cv.Mat(); // Error

            // Take first frame
            cap.read(oldGray);
            cv.cvtColor(oldGray, oldGray, cv.COLOR_RGBA2GRAY);

            // Find initial features to track
            findNewFeatures(oldGray);
            isTracking = true;
        }

        function findNewFeatures(grayMat) {
            let none = new cv.Mat();
            // goodFeaturesToTrack(image, outputPoints, maxCorners, qualityLevel, minDistance)
            cv.goodFeaturesToTrack(grayMat, p0, maxPoints, 0.3, 7, none);
            none.delete();
        }

        function processVideo() {
            if (!video || video.paused || video.ended) {
                requestAnimationFrame(processVideo);
                return;
            }

            if (!isTracking) {
                initTracking();
                requestAnimationFrame(processVideo);
                return;
            }

            if(!isCapturing) {
                // If in view mode, just render scene
                renderer.render(scene, camera);
                requestAnimationFrame(processVideo);
                return;
            }

            try {
                let frame = new cv.Mat(video.videoHeight, video.videoWidth, cv.CV_8UC4);
                cap.read(frame);
                cv.cvtColor(frame, frameGray, cv.COLOR_RGBA2GRAY);

                // Calculate Optical Flow (Lucas-Kanade)
                if (p0.rows > 0) {
                    let winSize = new cv.Size(15, 15);
                    let maxLevel = 2;
                    let criteria = new cv.TermCriteria(cv.TermCriteria_EPS | cv.TermCriteria_COUNT, 10, 0.03);
                    
                    cv.calcOpticalFlowPyrLK(oldGray, frameGray, p0, p1, st, err, winSize, maxLevel, criteria);

                    // Calculate average movement
                    let dx = 0, dy = 0, count = 0;
                    let goodNew = [];
                    
                    for (let i = 0; i < st.rows; i++) {
                        if (st.data[i] === 1) {
                            // Point found
                            goodNew.push(i);
                            let x0 = p0.data32F[i*2];
                            let y0 = p0.data32F[i*2+1];
                            let x1 = p1.data32F[i*2];
                            let y1 = p1.data32F[i*2+1];

                            dx += (x1 - x0);
                            dy += (y1 - y0);
                            count++;
                        }
                    }

                    document.getElementById('track-stats').innerText = `Tracking: ${count} points`;

                    // Apply Rotation to Camera (Fake Gyro)
                    if (count > 0) {
                        dx /= count;
                        dy /= count;

                        // Sensitivity Multiplier (Tune this for feel)
                        const sensitivity = 0.002; 
                        
                        // Invert logic: If pixels move left, camera turned right
                        camera.rotation.y -= dx * sensitivity; 
                        camera.rotation.x -= dy * sensitivity;
                    }

                    // Replenish points if too few
                    if (count < 10) {
                        findNewFeatures(frameGray);
                    } else {
                        // Swap p1 to p0 for next frame
                        // We need to copy valid points from p1 to p0
                        // For performance in JS, easier to just re-detect if count gets low, 
                        // but technically we should compact the arrays. 
                        // Simpler approach: simply update p0 with p1's data directly
                        p1.copyTo(p0);
                    }
                } else {
                    findNewFeatures(frameGray);
                }

                frameGray.copyTo(oldGray);
                frame.delete();

            } catch (err) {
                console.log(err);
                // Reset tracking on error
                isTracking = false;
            }

            renderer.render(scene, camera);
            requestAnimationFrame(processVideo);
        }

        // --- Capture Logic ---

        function captureFace() {
            // 1. Determine which face we are looking at
            const raycaster = new THREE.Raycaster();
            raycaster.setFromCamera(new THREE.Vector2(0, 0), camera);
            const intersects = raycaster.intersectObject(cubeMesh);

            if (intersects.length > 0) {
                const faceIndex = intersects[0].face.materialIndex;
                const material = materials[faceIndex];

                // 2. Capture Frame
                const canvas = document.createElement('canvas');
                // Use a power of 2 size
                const size = 1024;
                canvas.width = size; 
                canvas.height = size;
                const ctx = canvas.getContext('2d');

                // Crop the center square of video
                const vW = video.videoWidth;
                const vH = video.videoHeight;
                const minDim = Math.min(vW, vH);
                const sX = (vW - minDim) / 2;
                const sY = (vH - minDim) / 2;

                // Important: The cube faces are mirrored (inside view). 
                // We need to flip the image horizontally to match the "mirror"
                ctx.translate(size, 0);
                ctx.scale(-1, 1);
                ctx.drawImage(video, sX, sY, minDim, minDim, 0, 0, size, size);

                // 3. Apply Texture
                const tex = new THREE.CanvasTexture(canvas);
                tex.colorSpace = THREE.SRGBColorSpace;
                
                material.map = tex;
                material.opacity = 1.0; // Make opaque
                material.needsUpdate = true;

                // 4. Update UI
                if(!material.userData.filled) {
                    material.userData.filled = true;
                    capturedSegments++;
                }

                document.getElementById('msg').innerText = `Captured! (${capturedSegments}/6)`;
                
                // Visual Flash
                const flash = document.createElement('div');
                flash.style.position = 'fixed'; flash.style.top=0; flash.style.left=0; flash.style.width='100%'; flash.style.height='100%';
                flash.style.background = 'white'; flash.style.zIndex = 999;
                document.body.appendChild(flash);
                setTimeout(()=>flash.remove(), 100);

                // Show Finish button if user has at least captured something
                if(capturedSegments > 0) {
                    document.getElementById('view-controls').style.display = 'flex';
                    document.getElementById('capture-controls').style.display = 'none'; // Optional: hide capture until done? Or keep allow retake.
                    // Let's keep capture button unless fully done, but show finish button
                    document.getElementById('capture-controls').style.display = 'block';
                }
            }
        }

        // --- Finish & Local Storage ---

        function finishAndSave() {
            isCapturing = false;
            document.getElementById('msg').innerText = "Processing...";
            document.getElementById('ui').style.pointerEvents = 'none';

            // 1. Render the CubeMap to a single Equirectangular image
            // We use a CubeCamera to capture our scene
            const cubeRTarget = new THREE.WebGLCubeRenderTarget(1024);
            const cubeCam = new THREE.CubeCamera(0.1, 100, cubeRTarget);
            
            // Hide wireframe for export
            wireframeMesh.visible = false;
            scene.add(cubeCam);
            cubeCam.update(renderer, scene);
            
            // Shader to unwrap Cube -> Equirectangular
            const equiMat = new THREE.ShaderMaterial({
                uniforms: { map: { value: cubeRTarget.texture } },
                vertexShader: `
                    varying vec2 vUv;
                    void main() { vUv = uv; gl_Position = vec4(position, 1.0); }
                `,
                fragmentShader: `
                    uniform samplerCube map;
                    varying vec2 vUv;
                    const float PI = 3.14159265359;
                    void main() {
                        vec2 uv = vUv;
                        float longitude = uv.x * 2.0 * PI - PI;
                        float latitude = uv.y * PI - PI / 2.0;
                        vec3 dir = vec3(
                            -cos(latitude) * sin(longitude),
                            sin(latitude),
                            -cos(latitude) * cos(longitude)
                        );
                        gl_FragColor = textureCube(map, normalize(dir));
                    }
                `,
                side: THREE.DoubleSide
            });

            // Render to a flat texture
            const exportScene = new THREE.Scene();
            const plane = new THREE.Mesh(new THREE.PlaneGeometry(2, 2), equiMat);
            exportScene.add(plane);
            const orthoCam = new THREE.OrthographicCamera(-1, 1, 1, -1, 0, 1);
            
            renderer.setSize(2048, 1024); // Reasonable size for LocalStorage
            renderer.render(exportScene, orthoCam);

            // 2. Save to LocalStorage
            try {
                const dataUrl = renderer.domElement.toDataURL('image/jpeg', 0.8);
                localStorage.setItem('sphere_capture_data', dataUrl);
                alert("Sphere saved to Local Storage!");
                startViewerMode(dataUrl);
            } catch (e) {
                alert("Storage Error (Image too big): " + e);
            }
        }

        function startViewerMode(dataUrl) {
            // Clear current scene
            while(scene.children.length > 0){ 
                scene.remove(scene.children[0]); 
            }
            
            // Switch to Viewer UI
            document.getElementById('ui').style.pointerEvents = 'auto';
            document.getElementById('capture-controls').style.display = 'none';
            document.getElementById('view-controls').style.display = 'none'; // Hide finish btn
            document.getElementById('msg').innerText = "Viewer Mode - Drag to Pan";
            document.getElementById('track-stats').style.display = 'none';
            document.getElementById('three-canvas').style.pointerEvents = 'auto'; // Enable drag

            // Stop Camera
            video.pause();
            video.srcObject.getTracks().forEach(track => track.stop());

            // Reset Renderer size
            renderer.setSize(window.innerWidth, window.innerHeight);
            scene.background = new THREE.Color(0x111111);

            // Create Sphere with saved image
            const texture = new THREE.TextureLoader().load(dataUrl);
            texture.colorSpace = THREE.SRGBColorSpace;
            const geometry = new THREE.SphereGeometry(10, 60, 40);
            const material = new THREE.MeshBasicMaterial({ map: texture, side: THREE.BackSide });
            const sphere = new THREE.Mesh(geometry, material);
            // Fix orientation for sphere
            sphere.scale.set(-1, 1, 1);
            scene.add(sphere);

            // Orbit Controls for viewing
            camera.position.set(0,0,0.1);
            const controls = new OrbitControls(camera, renderer.domElement);
            controls.enableZoom = false;
            controls.enablePan = false;
            controls.rotateSpeed = -0.5; // Invert for "inside" feel
            
            // Render loop for viewer
            renderer.setAnimationLoop(() => {
                controls.update();
                renderer.render(scene, camera);
            });
        }
    </script>
</body>
</html>
